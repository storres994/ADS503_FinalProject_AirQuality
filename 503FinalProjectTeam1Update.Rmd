---
title: "ADS 503 Final Project"
author: 'Group1: Sean Torres, George Garcia, and Anusia Edward'
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pls)
library(corrplot)
library(caret)
library(pROC)
library(VIM)
```

```{r, message=FALSE}
air <- read.csv("~/Desktop/Shunyi.csv")
```

```{r, message=FALSE}
sum(is.na(air)) # checking for NAs 

air.knn <- kNN(air) # using 5KNN to impute missing values 
# removing the imputation truth variables that were added 
air.knn <- subset(air.knn, select = year:station)
sum(is.na(air.knn)) # double checking NAs 
```

```{r, message=FALSE}
# splitting predictors and outcome variables 
x <- subset(air.knn, select = -c(PM2.5))
y <- subset(air.knn, select = PM2.5)
library(caret)
nZV.x <- nearZeroVar(x)# checking near zero variance 
x <- x[, -nZV.x]# removal of nZV 
# splitting the data before preprocessing to avoid data leakage
library(caret) 
set.seed(1)
trainset <- createDataPartition(air.knn$PM2.5, p = 0.8, list = FALSE)
x.train <- x[trainset, ]
y.train <- y[trainset, ]
y.train1 <- as.data.frame(y.train)
x.test <- x[-trainset, ]
y.test <- y[-trainset, ]
y.test1 <- as.data.frame(y.test)
```

```{r, message=FALSE}
# checking distributions
par(mfrow = c(2,3))
hist(x.train$year,xlab = "Year")
hist(x.train$month, xlab = "Month")
hist(x.train$day, xlab = "Day")
hist(x.train$hour, xlab = "Hour")
hist(x.train$PM10, xlab = "PM10")
hist(x.train$SO2, xlab = "SO2")
par(mfrow = c(2,4))
hist(x.train$NO2, xlab = "NO2")
hist(x.train$CO, xlab = "CO")
hist(x.train$O3, xlab = "O3")
hist(x.train$TEMP, xlab = "Temperature")
hist(x.train$PRES, xlab = "Pressure")
hist(x.train$DEWP, xlab = "Dew point Temp")
hist(x.train$WSPM, xlab = "Wind Speed")

```

```{r, message=FALSE}
# box-cox, center, scaling 
trans <- preProcess(x.train, 
                       method = c("BoxCox", "center", "scale"))
x.trainp <- predict(trans, x.train)
trans1 <- preProcess(y.train1, 
                       method = c("BoxCox", "center", "scale"))
y.trainp <- predict(trans1, y.train1)
trans2 <- preProcess(x.test, 
                       method = c("BoxCox", "center", "scale"))
x.testp <- predict(trans2, x.test)
trans3 <- preProcess(y.test1, 
                       method = c("BoxCox", "center", "scale"))
y.testp <- predict(trans3, y.test1)

# visualization of the transformations 
par(mfrow = c(2,2))
hist(x.train$CO, xlab = "CO")
hist(x.trainp$CO, xlab = "CO after tranformations")
hist(x.train$O3, xlab = "O3")
hist(x.trainp$O3, xlab = "O3 after transformations")
```

```{r, message=FALSE}
# checking for correlations 
x.corr <- cor(x.trainp)
library(corrplot)
corrplot(x.corr, order = "hclust")
hCorr <- findCorrelation(x.corr, cutoff = 0.75, exact = TRUE)
x.trainpc <- x.trainp[, -hCorr]
x.testpc <- x.testp[, -hCorr]
x.corrCheck <- cor(x.trainpc)
x.corrCheck 
corrplot(x.corrCheck, order = "hclust")
```





```{r}
# pca to determine the effective dimensions of the data 
pca.x <- prcomp(x.train, center = TRUE, scale. = TRUE)
summary(pca.x)
plot(pca.x, type = "l")
```

```{r}

df = subset(air.knn, select = -c(station) )
pca <- prcomp(df, scale = TRUE)
pca
variance = pca$sdev^2 / sum(pca$sdev^2)
#variance
library(ggplot2)
qplot(c(1:15), variance) +
  geom_line() +
  geom_point(size=4)+
  xlab("Principal Component") +
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)
```



```{r OLS}
#Using as a base model that is simple
set.seed(100)
indx <- createFolds(y.train, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)
pcrTune2 <- train(x = x.trainp, y = y.train,
                 method = "lm",trControl = ctrl)
pcrTune2  
summary(pcrTune2)
#testResults 

# rfImp <- varImp(pcrTune2, scale = FALSE)
#rfImp

fp_predict <- predict(pcrTune2, x.testp)

postResample(fp_predict, y.test)
#Taking account of RMSE and Rsqr values OLS seems to be the better model.
# Although it tied with pls ols is the simpler model. 
```


```{r}
# try to reduce features using pls
set.seed(100)
indx <- createFolds(y.train, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)
pcrTune3 <- train(x = x.trainp, y = y.train,
                 method = "pls",
                 tuneGrid = expand.grid(ncomp = 1:14),
                 trControl = ctrl)
pcrTune3
summary(pcrTune3) 


fp_predict <- predict(pcrTune3, x.testp)

postResample(fp_predict, y.test)

#rfImp <- varImp(pcrTune3, scale = FALSE)
#rfImp
```



```{r}
# great for large data decision trees will be having better average accuracy.
library(randomForest)
rfmodel <- randomForest(x = x.train,y = y.train,importance=TRUE,ntrees=500)


getRMSE <- function(x,y) {
  sqrt(sum((x-y)^2)/length(x))
}


testResults <- data.frame(obs = y.test,
                          rfmodel = predict(rfmodel, x.test))
getRMSE(testResults$obs, testResults$rfmodel)

fp_predict <- predict(rfmodel , x.testp)

postResample(fp_predict, y.test)
#rfImp <- varImp(rfmodel, scale = FALSE)
#rfImp
```







```{r}
resamp <- resamples(list(OLS = pcrTune2, PLS = pcrTune3))
summary(resamp)
```
